"""
åŸºæº–ç·šç®¡ç†åŠŸèƒ½ - æ”¯æ´ LZ4ã€Zstd å’Œ gzip å£“ç¸®
"""
import os
import json
import gzip
import shutil
import time
import gc
import threading
from datetime import datetime, timedelta
import logging
import config.settings as settings
from utils.helpers import save_progress, load_progress
from utils.memory import check_memory_limit, get_memory_usage
from utils.compression import (
    CompressionFormat, 
    save_compressed_file, 
    load_compressed_file,
    get_compression_stats,
    migrate_baseline_format
)
from core.excel_parser import dump_excel_cells_with_timeout, hash_excel_content, get_excel_last_author

def baseline_file_path(base_name):
    """
    ç²å–åŸºæº–ç·šæª”æ¡ˆè·¯å¾‘ï¼ˆä¸åŒ…å«å‰¯æª”åï¼‰
    """
    return os.path.join(settings.LOG_FOLDER, f"{base_name}.baseline.json")

def get_baseline_file_with_extension(base_name):
    """
    ç²å–å¯¦éš›å­˜åœ¨çš„åŸºæº–ç·šæª”æ¡ˆè·¯å¾‘ï¼ˆåŒ…å«å‰¯æª”åï¼‰
    """
    base_path = baseline_file_path(base_name)
    
    # æŒ‰å„ªå…ˆé †åºæª¢æŸ¥ä¸åŒæ ¼å¼çš„æª”æ¡ˆ
    for format_type in [settings.DEFAULT_COMPRESSION_FORMAT, 'lz4', 'zstd', 'gzip']:
        ext = CompressionFormat.get_extension(format_type)
        test_path = base_path + ext
        if os.path.exists(test_path):
            return test_path
    
    return None

def load_baseline(baseline_file_or_base_name):
    """
    è¼‰å…¥åŸºæº–ç·šæª”æ¡ˆï¼Œæ”¯æ´å¤šç¨®å£“ç¸®æ ¼å¼
    """
    try:
        # å¦‚æœæ˜¯åŸºæº–åç¨±ï¼Œè½‰æ›ç‚ºæª”æ¡ˆè·¯å¾‘
        if not os.path.sep in baseline_file_or_base_name and not baseline_file_or_base_name.endswith('.json'):
            base_path = baseline_file_path(baseline_file_or_base_name)
        else:
            base_path = baseline_file_or_base_name
            if base_path.endswith('.gz') or base_path.endswith('.lz4') or base_path.endswith('.zst'):
                base_path = base_path.rsplit('.', 1)[0]
        
        # ä½¿ç”¨å£“ç¸®å·¥å…·è¼‰å…¥
        from utils.compression import load_compressed_file
        data = load_compressed_file(base_path)
        
        # ç§»é™¤æ‰€æœ‰ [DEBUG] è¼‰å…¥åŸºæº–ç·šçš„è¨Šæ¯
        
        return data
        
    except (FileNotFoundError, PermissionError, OSError, json.JSONDecodeError, gzip.BadGzipFile) as e:
        logging.error(f"è¼‰å…¥åŸºæº–ç·šå¤±æ•— {baseline_file_or_base_name}: {e}")
        return None

def save_baseline(baseline_file_or_base_name, data):
    """
    ä¿å­˜åŸºæº–ç·šæª”æ¡ˆï¼Œä½¿ç”¨è¨­å®šçš„å£“ç¸®æ ¼å¼
    """
    try:
        # å¦‚æœæ˜¯åŸºæº–åç¨±ï¼Œè½‰æ›ç‚ºæª”æ¡ˆè·¯å¾‘
        if not os.path.sep in baseline_file_or_base_name and not baseline_file_or_base_name.endswith('.json'):
            base_path = baseline_file_path(baseline_file_or_base_name)
        else:
            base_path = baseline_file_or_base_name
            if base_path.endswith('.gz') or base_path.endswith('.lz4') or base_path.endswith('.zst'):
                base_path = base_path.rsplit('.', 1)[0]
        
        # ç§»é™¤ï¼š print(f"[DEBUG] åŸºæº–è·¯å¾‘: {base_path}")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        dir_name = os.path.dirname(base_path)
        os.makedirs(dir_name, exist_ok=True)
        
        # ä½¿ç”¨æ–°çš„å£“ç¸®å·¥å…·
        from utils.compression import save_compressed_file, get_compression_stats, CompressionFormat
        
        # é¸æ“‡å£“ç¸®æ ¼å¼
        compression_format = settings.DEFAULT_COMPRESSION_FORMAT
        # ç§»é™¤ï¼š print(f"[DEBUG] ä½¿ç”¨æ ¼å¼: {compression_format}")
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†èˆŠæ ¼å¼çš„æª”æ¡ˆ
        for old_format in ['gzip', 'lz4', 'zstd']:
            if old_format != compression_format:
                old_ext = CompressionFormat.get_extension(old_format)
                old_file = base_path + old_ext
                if os.path.exists(old_file):
                    try:
                        os.remove(old_file)
                    except OSError as e:
                        logging.warning(f"æ¸…ç†èˆŠæª”æ¡ˆå¤±æ•—: {e}")
        
        # ä¿å­˜æ–°æª”æ¡ˆ
        # ç§»é™¤ï¼š print(f"[DEBUG] é–‹å§‹ä¿å­˜å£“ç¸®æª”æ¡ˆ...")
        actual_file = save_compressed_file(base_path, data, compression_format)
        # ç§»é™¤ï¼š print(f"[DEBUG] ä¿å­˜å®Œæˆ: {actual_file}")
        
        # ç°¡åŒ–å£“ç¸®çµ±è¨ˆé¡¯ç¤º
        if settings.SHOW_COMPRESSION_STATS:
            stats = get_compression_stats(actual_file)
            if stats:
                print(f"åŸºæº–ç·šä¿å­˜: {os.path.basename(actual_file)} ({stats['format'].upper()}, {stats['compression_ratio']:.1f}%)")
        
        return True
        
    except (FileNotFoundError, PermissionError, OSError) as e:
        logging.error(f"ä¿å­˜åŸºæº–ç·šæª”æ¡ˆå¤±æ•—: {e}")
        return False

def archive_old_baselines():
    """
    æ­¸æª”èˆŠçš„åŸºæº–ç·šæª”æ¡ˆï¼Œè½‰æ›ç‚ºé«˜å£“ç¸®ç‡æ ¼å¼
    """
    if not settings.ENABLE_ARCHIVE_MODE:
        return
    
    try:
        archive_threshold = datetime.now() - timedelta(days=settings.ARCHIVE_AFTER_DAYS)
        archive_count = 0
        
        for filename in os.listdir(settings.LOG_FOLDER):
            if not filename.endswith('.baseline.json.lz4'):
                continue
            
            filepath = os.path.join(settings.LOG_FOLDER, filename)
            file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))
            
            if file_mtime < archive_threshold:
                print(f"[ARCHIVE] æ­¸æª”èˆŠåŸºæº–ç·š: {filename}")
                new_filepath = migrate_baseline_format(filepath, settings.ARCHIVE_COMPRESSION_FORMAT)
                if new_filepath:
                    archive_count += 1
                    print(f"[ARCHIVE] å®Œæˆ: {os.path.basename(new_filepath)}")
        
        if archive_count > 0:
            print(f"[ARCHIVE] å…±æ­¸æª”äº† {archive_count} å€‹åŸºæº–ç·šæª”æ¡ˆ")
    
    except (OSError, shutil.Error) as e:
        logging.error(f"æ­¸æª”éç¨‹å‡ºéŒ¯: {e}")

def create_baseline_for_files_robust(xlsx_files, skip_force_baseline=True):
    """
    ç‚ºå¤šå€‹æª”æ¡ˆå»ºç«‹åŸºæº–ç·š
    """
    total = len(xlsx_files)
    if total == 0:
        print("[INFO] æ²’æœ‰éœ€è¦ baseline çš„æª”æ¡ˆã€‚")
        settings.baseline_completed = True
        return
    
    print("\n" + "="*90 + "\n" + "BASELINE å»ºç«‹ç¨‹åº".center(90) + "\n" + "="*90)
    
    # æª¢æŸ¥å£“ç¸®æ ¼å¼å¯ç”¨æ€§
    available_formats = CompressionFormat.get_available_formats()
    print(f"ğŸ—œï¸  å¯ç”¨å£“ç¸®æ ¼å¼: {', '.join(available_formats)}")
    print(f"ğŸš€ ä½¿ç”¨å£“ç¸®æ ¼å¼: {settings.DEFAULT_COMPRESSION_FORMAT.upper()}")
    
    if settings.DEFAULT_COMPRESSION_FORMAT not in available_formats:
        print(f"âš ï¸  è­¦å‘Š: é è¨­æ ¼å¼ {settings.DEFAULT_COMPRESSION_FORMAT} ä¸å¯ç”¨ï¼Œé™ç´šåˆ° gzip")
        settings.DEFAULT_COMPRESSION_FORMAT = 'gzip'
    
    progress = load_progress()
    start_index = 0
    
    if progress and settings.ENABLE_RESUME:
        print(f"ç™¼ç¾ä¹‹å‰çš„é€²åº¦è¨˜éŒ„: å®Œæˆ {progress.get('completed', 0)}/{progress.get('total', 0)}")
        # è‡ªå‹•çºŒè·‘ï¼šä¸å†è©¢å•
        start_index = progress.get('completed', 0)
    
    # å•Ÿå‹•è¶…æ™‚è™•ç†
    if settings.ENABLE_TIMEOUT:
        from utils.helpers import timeout_handler
        timeout_thread = threading.Thread(target=timeout_handler, daemon=True)
        timeout_thread.start()
        print(f"â° å•Ÿç”¨è¶…æ™‚ä¿è­·: {settings.FILE_TIMEOUT_SECONDS} ç§’")
    
    if settings.ENABLE_MEMORY_MONITOR: 
        print(f"ğŸ’¾ å•Ÿç”¨è¨˜æ†¶é«”ç›£æ§: {settings.MEMORY_LIMIT_MB} MB")
    
    print(f"ğŸš€ å•Ÿç”¨å„ªåŒ–: {[opt for flag, opt in [(settings.USE_LOCAL_CACHE, 'æœ¬åœ°ç·©å­˜'), (settings.ENABLE_FAST_MODE, 'å¿«é€Ÿæ¨¡å¼')] if flag]}")
    print(f"ğŸ“‚ Baseline å„²å­˜ä½ç½®: {os.path.abspath(settings.LOG_FOLDER)}")
    
    if settings.USE_LOCAL_CACHE: 
        print(f"ğŸ’¾ æœ¬åœ°ç·©å­˜ä½ç½®: {os.path.abspath(settings.CACHE_FOLDER)}")
    
    print(f"ğŸ“‹ è¦è™•ç†çš„æª”æ¡ˆ: {total} å€‹ (å¾ç¬¬ {start_index + 1} å€‹é–‹å§‹)")
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.now():%Y-%m-%d %H:%M:%S}\n" + "-"*90)
    
    os.makedirs(settings.LOG_FOLDER, exist_ok=True)
    if settings.USE_LOCAL_CACHE: 
        os.makedirs(settings.CACHE_FOLDER, exist_ok=True)
    
    success_count, skip_count, error_count = 0, 0, 0
    start_time = time.time()
    total_original_size = 0
    total_compressed_size = 0
    
    for i in range(start_index, total):
        if settings.force_stop:
            print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡è™Ÿï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
            save_progress(i, total)
            break
        
        file_path = xlsx_files[i]
        # ä½¿ç”¨åŒ…å«è·¯å¾‘å“ˆå¸Œçš„ keyï¼Œé¿å…åŒåä¸åŒè·¯å¾‘è¦†è“‹
        from utils.helpers import _baseline_key_for_path
        base_key = _baseline_key_for_path(file_path)
        display_name = os.path.basename(file_path)
        
        if check_memory_limit():
            print(f"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨é‡éé«˜ï¼Œæš«åœ10ç§’...")
            time.sleep(10)
            if check_memory_limit(): 
                print(f"âŒ è¨˜æ†¶é«”ä»ç„¶éé«˜ï¼Œåœæ­¢è™•ç†")
                save_progress(i, total)
                break

        file_start_time = time.time()
        print(f"[{i+1:>2}/{total}] è™•ç†ä¸­: {display_name} (è¨˜æ†¶é«”: {get_memory_usage():.1f}MB)")
        
        cell_data = None
        try:
            old_baseline = load_baseline(base_key)
            old_hash = old_baseline['content_hash'] if old_baseline and 'content_hash' in old_baseline else None
            
            cell_data = dump_excel_cells_with_timeout(file_path)
            
            if cell_data is None:
                if settings.current_processing_file is None and (time.time() - file_start_time) > settings.FILE_TIMEOUT_SECONDS:
                     print(f"  çµæœ: [TIMEOUT]")
                else:
                     print(f"  çµæœ: [READ_ERROR]")
                error_count += 1
            else:
                curr_hash = hash_excel_content(cell_data)
                if (not getattr(settings, 'FORCE_REBUILD_BASELINE_ON_SCAN', False)) and old_hash == curr_hash and old_hash is not None:
                    print(f"  çµæœ: [SKIP] (Hash unchanged)")
                    skip_count += 1
                else:
                    curr_author = get_excel_last_author(file_path)
                    # è¨˜éŒ„ä¸‰ç¨®æ™‚é–“ï¼š
                    # 1. timestamp: å»ºç«‹ baseline çš„è™•ç†æ™‚é–“
                    # 2. source_mtime: å»ºç«‹ baseline æ™‚æª”æ¡ˆçš„ä¿®æ”¹æ™‚é–“  
                    # 3. file_mtime_str: æ ¼å¼åŒ–çš„æª”æ¡ˆä¿®æ”¹æ™‚é–“ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰
                    file_mtime = os.path.getmtime(file_path)
                    baseline_data = {
                        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # è™•ç†æ™‚é–“
                        "source_mtime": file_mtime,  # æª”æ¡ˆä¿®æ”¹æ™‚é–“æˆ³
                        "file_mtime_str": datetime.fromtimestamp(file_mtime).strftime('%Y-%m-%d %H:%M:%S'),  # æ ¼å¼åŒ–æª”æ¡ˆä¿®æ”¹æ™‚é–“
                        "source_size": os.path.getsize(file_path),
                        "last_author": curr_author, 
                        "content_hash": curr_hash, 
                        "cells": cell_data
                    }
                    
                    if save_baseline(base_key, baseline_data):
                        print(f"  çµæœ: [OK]")
                        success_count += 1
                        
                        # çµ±è¨ˆå£“ç¸®æ•ˆæœ
                        if settings.SHOW_COMPRESSION_STATS:
                            actual_file = get_baseline_file_with_extension(base_key)
                            if actual_file:
                                stats = get_compression_stats(actual_file)
                                if stats and stats['original_size']:
                                    total_original_size += stats['original_size']
                                    total_compressed_size += stats['compressed_size']
                    else:
                        print(f"  çµæœ: [SAVE_ERROR]")
                        error_count += 1
            
            print(f"  è€—æ™‚: {time.time() - file_start_time:.2f} ç§’")
            print("")
            save_progress(i + 1, total)
            
        except (FileNotFoundError, PermissionError, OSError, json.JSONDecodeError) as e:
            logging.error(f"  çµæœ: [UNEXPECTED_ERROR]\n  éŒ¯èª¤: {e}\n  è€—æ™‚: {time.time() - file_start_time:.2f} ç§’\n")
            error_count += 1
            save_progress(i + 1, total)
        finally:
            if cell_data is not None:
                del cell_data
            if 'old_baseline' in locals() and old_baseline is not None:
                del old_baseline
            # ç§»é™¤é¡¯å¼ gc.collect() ä»¥é¿å…åœ¨ GC éšæ®µè§¸ç™¼åº•å±¤å´©æ½°ï¼ˆPython 3.11/3.12 + ElementTreeï¼‰
            # äº¤ç”± Python è‡ªç„¶å›æ”¶å³å¯ï¼Œå¿…è¦æ™‚å¯æ”¹å›è¨­å®šå¼é–‹é—œ

    # åŸ·è¡Œæ­¸æª”
    if settings.ENABLE_ARCHIVE_MODE:
        print("\nğŸ—‚ï¸  æª¢æŸ¥æ­¸æª”...")
        archive_old_baselines()

    settings.baseline_completed = True
    print("-" * 90 + f"\nğŸ¯ BASELINE å»ºç«‹å®Œæˆ! (ç¸½è€—æ™‚: {time.time() - start_time:.2f} ç§’)")
    print(f"âœ… æˆåŠŸ: {success_count}, â­ï¸  è·³é: {skip_count}, âŒ å¤±æ•—: {error_count}")
    
    # é¡¯ç¤ºå£“ç¸®çµ±è¨ˆ
    if settings.SHOW_COMPRESSION_STATS and total_original_size > 0:
        overall_ratio = (1 - total_compressed_size / total_original_size) * 100
        savings_mb = (total_original_size - total_compressed_size) / (1024 * 1024)
        print(f"ğŸ—œï¸  ç¸½å£“ç¸®çµ±è¨ˆ: åŸå§‹ {total_original_size/(1024*1024):.1f}MB â†’ "
              f"å£“ç¸® {total_compressed_size/(1024*1024):.1f}MB "
              f"(ç¯€çœ {savings_mb:.1f}MB, å£“ç¸®ç‡ {overall_ratio:.1f}%)")
    
    if settings.ENABLE_RESUME and os.path.exists(settings.RESUME_LOG_FILE):
        try: 
            os.remove(settings.RESUME_LOG_FILE)
            print(f"ğŸ§¹ æ¸…ç†é€²åº¦æª”æ¡ˆ")
        except OSError as e:
            logging.error(f"æ¸…ç†é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
    
    print("\n" + "=" * 90 + "\n")

# --- Subprocess-only override for baseline I/O ---

def _baseline__resolve_actual_file(arg):
    import os
    if (os.path.sep not in arg) and (not str(arg).endswith('.json')):
        try:
            return get_baseline_file_with_extension(arg)
        except Exception:
            return None
    return arg

def _load_baseline_subproc_only(baseline_file_or_base_name):
    """Load baseline strictly via subprocess (no fallback)."""
    import os
    actual = _baseline__resolve_actual_file(baseline_file_or_base_name)
    if not actual or not os.path.exists(actual):
        return None
    try:
        from utils.subprocess_manager import get_subprocess_manager
        mgr = get_subprocess_manager()
        result = mgr.load_baseline_safe(actual)
        if result.get('success'):
            return result.get('baseline_data', {}) or {}
    except Exception as e:
        # no fallback
        pass
    return None

def _save_baseline_subproc_only(baseline_file_or_base_name, data):
    """Save baseline strictly via subprocess (no fallback)."""
    import os
    from utils.compression import CompressionFormat, get_compression_stats
    # resolve base path without extension
    if (os.path.sep not in baseline_file_or_base_name) and (not str(baseline_file_or_base_name).endswith('.json')):
        base_path = baseline_file_path(baseline_file_or_base_name)
    else:
        base_path = baseline_file_or_base_name
        if base_path.endswith('.gz') or base_path.endswith('.lz4') or base_path.endswith('.zst'):
            base_path = base_path.rsplit('.', 1)[0]
    # ensure dir
    try:
        os.makedirs(os.path.dirname(base_path), exist_ok=True)
    except Exception:
        pass
    # build target by format
    fmt = getattr(settings, 'DEFAULT_COMPRESSION_FORMAT', 'lz4')
    ext = CompressionFormat.get_extension(fmt)
    target = base_path + ext
    try:
        from utils.subprocess_manager import get_subprocess_manager
        mgr = get_subprocess_manager()
        ok = mgr.save_baseline_safe(target, data, fmt)
        if ok and getattr(settings, 'SHOW_COMPRESSION_STATS', False):
            try:
                stats = get_compression_stats(target)
                if stats:
                    print(f"åŸºæº–ç·šä¿å­˜: {os.path.basename(target)} ({stats['format'].upper()}, {stats['compression_ratio']:.1f}%)")
            except Exception:
                pass
        return bool(ok)
    except Exception:
        return False

# enforce override (no fallback)
load_baseline = _load_baseline_subproc_only
save_baseline = _save_baseline_subproc_only